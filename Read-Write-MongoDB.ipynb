{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e6c4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 02:25:38.817881: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-20 02:25:39.130683: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-20 02:25:40.541824: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-20 02:25:44.162805: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "import random\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col, length, split, lit, when, concat, isnull, mode, isnan, count, lower, upper, sum, first, regexp_replace, concat_ws, year, hour, minute, month, to_date, to_timestamp\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import googletrans\n",
    "from googletrans import *\n",
    "from langdetect import detect\n",
    "\n",
    "from operator import add\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea63e1a0",
   "metadata": {},
   "source": [
    "## Read from hadoop with Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d8f49b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('/CA2/ProjectTweets.csv', header=False).toDF(\"Indx\",\"ID\", \"DATE\", \"FLAG\", \"USER\", \"TWEET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a180cfb",
   "metadata": {},
   "source": [
    "## Write to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "654f4691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Data inserted\n"
     ]
    }
   ],
   "source": [
    "#df = spark.read.csv('/CA2/ProjectTweets.csv', header=False).toDF(\"Indx\",\"ID\", \"DATE\", \"FLAG\", \"USER\", \"TWEET\")\n",
    "# Start Spark Session\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"CSV to MongoDB\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# Define MongoDB connection\n",
    "mongo_uri = \"mongodb://localhost:27017/\"\n",
    "database_name = \"Tweets\"\n",
    "collection_name = \"tweets_collection\"\n",
    "\n",
    "# Convert spark df in a MongoDB readable format\n",
    "data = df.toPandas().to_dict(orient='records')\n",
    "\n",
    "# Connection to MongoDB\n",
    "client = MongoClient(mongo_uri)\n",
    "db = client[database_name]\n",
    "collection = db[collection_name]\n",
    "\n",
    "# Insert to MongoDB\n",
    "collection.insert_many(data)\n",
    "print('\\n')\n",
    "print('Data inserted')\n",
    "\n",
    "# Close connection to MongoDB\n",
    "client.close()\n",
    "\n",
    "# Stop Spark Session\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88013ff2",
   "metadata": {},
   "source": [
    "## Read from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b0a9155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------+----+--------------------+---------------+--------------------+\n",
      "|                DATE|    FLAG|        ID|Indx|               TWEET|           USER|                 _id|\n",
      "+--------------------+--------+----------+----+--------------------+---------------+--------------------+\n",
      "|Mon Apr 06 22:19:...|NO_QUERY|1467810369|   0|@switchfoot http:...|_TheSpecialOne_|{664aa714d56681f7...|\n",
      "|Mon Apr 06 22:19:...|NO_QUERY|1467810672|   1|is upset that he ...|  scotthamilton|{664aa714d56681f7...|\n",
      "|Mon Apr 06 22:19:...|NO_QUERY|1467810917|   2|@Kenichan I dived...|       mattycus|{664aa714d56681f7...|\n",
      "|Mon Apr 06 22:19:...|NO_QUERY|1467811184|   3|my whole body fee...|        ElleCTF|{664aa714d56681f7...|\n",
      "|Mon Apr 06 22:19:...|NO_QUERY|1467811193|   4|@nationwideclass ...|         Karoli|{664aa714d56681f7...|\n",
      "+--------------------+--------+----------+----+--------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# # Start Spark Session\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"MongoDB to PySpark\") \\\n",
    "#     .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/Tweets.tweets_collection\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "mongo_uri = \"mongodb://localhost:27017/Tweets.tweets_collection\"\n",
    "\n",
    "# Read from MongoDB\n",
    "dfM = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\", mongo_uri).load()\n",
    "#df1 = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "\n",
    "# Check df\n",
    "dfM.show(5)\n",
    "\n",
    "# Check number of rows\n",
    "print(dfM.count())\n",
    "\n",
    "# Stop Spark Session\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e7717da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dfM.alias('df1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b01c914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- FLAG: string (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Indx: string (nullable = true)\n",
      " |-- TWEET: string (nullable = true)\n",
      " |-- USER: string (nullable = true)\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aadd73",
   "metadata": {},
   "source": [
    "## Data cleaning using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "567cd740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------+----+--------------------+---------------+--------------------+\n",
      "|                DATE|    FLAG|        ID|Indx|               TWEET|           USER|                 _id|\n",
      "+--------------------+--------+----------+----+--------------------+---------------+--------------------+\n",
      "|Mon Apr 06 22:19:...|NO_QUERY|1467810369|   0|@switchfoot http:...|_TheSpecialOne_|{664aa714d56681f7...|\n",
      "|Mon Apr 06 22:19:...|NO_QUERY|1467810672|   1|is upset that he ...|  scotthamilton|{664aa714d56681f7...|\n",
      "|Mon Apr 06 22:19:...|NO_QUERY|1467810917|   2|@Kenichan I dived...|       mattycus|{664aa714d56681f7...|\n",
      "|Mon Apr 06 22:19:...|NO_QUERY|1467811184|   3|my whole body fee...|        ElleCTF|{664aa714d56681f7...|\n",
      "|Mon Apr 06 22:19:...|NO_QUERY|1467811193|   4|@nationwideclass ...|         Karoli|{664aa714d56681f7...|\n",
      "+--------------------+--------+----------+----+--------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- FLAG: string (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Indx: string (nullable = true)\n",
      " |-- TWEET: string (nullable = true)\n",
      " |-- USER: string (nullable = true)\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# column_order = [\"Indx\", \"ID\", \"DATE\", \"FLAG\", \"USER\", \"TWEET\"]\n",
    "\n",
    "# df1 = df1.select(column_order)\n",
    "\n",
    "# df1 = df1.dropDuplicates().withColumn(\"Indx\", df1[\"Indx\"].cast(\"int\")).orderBy(col(\"Indx\"))\n",
    "\n",
    "df1.show(5)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90b65499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets with more than 140 characters: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-------------------+-----------+--------------------+------------+\n",
      "|Indx|        ID|               DATE|DAY_OF_WEEK|               TWEET|tweet_length|\n",
      "+----+----------+-------------------+-----------+--------------------+------------+\n",
      "|   0|1467810369|2009-04-06 22:19:45|         01|   awww thats a b...|          74|\n",
      "|   1|1467810672|2009-04-06 22:19:49|         01|is upset that he ...|         105|\n",
      "|   2|1467810917|2009-04-06 22:19:53|         01| i dived many tim...|          76|\n",
      "|   3|1467811184|2009-04-06 22:19:57|         01|my whole body fee...|          47|\n",
      "|   4|1467811193|2009-04-06 22:19:57|         01| no its not behav...|          87|\n",
      "+----+----------+-------------------+-----------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "column_order = [\"Indx\", \"ID\", \"DATE\", \"FLAG\", \"USER\", \"TWEET\"]\n",
    "\n",
    "df1 = df1.select(column_order)\n",
    "\n",
    "df1 = df1.dropDuplicates().withColumn(\"Indx\", df1[\"Indx\"].cast(\"int\")).orderBy(col(\"Indx\"))\n",
    "\n",
    "# Split Date column\n",
    "df1 = df1.withColumn(\"Day_of_week\", split(df1[\"DATE\"], \" \")[0]) \\\n",
    "         .withColumn(\"Month\", split(df1[\"DATE\"], \" \")[1]) \\\n",
    "         .withColumn(\"Day\", split(df1[\"DATE\"], \" \")[2]) \\\n",
    "         .withColumn(\"Time\", split(df1[\"DATE\"], \" \")[3]) \\\n",
    "         .withColumn(\"Timezone\", split(df1[\"DATE\"], \" \")[4]) \\\n",
    "         .withColumn(\"Year\", split(df1[\"DATE\"], \" \")[5])\n",
    "\n",
    "# Map Months\n",
    "m_map = {'Jan': '01', 'Feb': '02', 'Mar': '03',\n",
    "         'Apr': '04', 'May': '05', 'Jun': '06',\n",
    "         'Jul': '07', 'Aug': '08', 'Sep': '09',\n",
    "         'Oct': '10', 'Nov': '11', 'Dec': '12'}\n",
    "\n",
    "df1 = df1.withColumn(\"MONTH\", \n",
    "                     when(df1[\"MONTH\"] == \"Jan\", \"01\")\n",
    "                     .when(df1[\"MONTH\"] == \"Feb\", \"02\")\n",
    "                     .when(df1[\"MONTH\"] == \"Mar\", \"03\")\n",
    "                     .when(df1[\"MONTH\"] == \"Apr\", \"04\")\n",
    "                     .when(df1[\"MONTH\"] == \"May\", \"05\")\n",
    "                     .when(df1[\"MONTH\"] == \"Jun\", \"06\")\n",
    "                     .when(df1[\"MONTH\"] == \"Jul\", \"07\")\n",
    "                     .when(df1[\"MONTH\"] == \"Aug\", \"08\")\n",
    "                     .when(df1[\"MONTH\"] == \"Sep\", \"09\")\n",
    "                     .when(df1[\"MONTH\"] == \"Oct\", \"10\")\n",
    "                     .when(df1[\"MONTH\"] == \"Nov\", \"11\")\n",
    "                     .when(df1[\"MONTH\"] == \"Dec\", \"12\")\n",
    "                     .otherwise(df1[\"MONTH\"]))\n",
    "\n",
    "# Maap weekdays\n",
    "d_map = {'Mon': '01', 'Tue': '02', 'Wed': '03',\n",
    "         'Thu': '04', 'Fri': '05', 'Sat': '06',\n",
    "         'Sun': '07'}\n",
    "\n",
    "df1 = df1.withColumn(\"DAY_OF_WEEK\", \n",
    "                     when(df1[\"DAY_OF_WEEK\"] == \"Mon\", \"01\")\n",
    "                     .when(df1[\"DAY_OF_WEEK\"] == \"Tue\", \"02\")\n",
    "                     .when(df1[\"DAY_OF_WEEK\"] == \"Wed\", \"03\")\n",
    "                     .when(df1[\"DAY_OF_WEEK\"] == \"Thu\", \"04\")\n",
    "                     .when(df1[\"DAY_OF_WEEK\"] == \"Fri\", \"05\")\n",
    "                     .when(df1[\"DAY_OF_WEEK\"] == \"Sat\", \"06\")\n",
    "                     .when(df1[\"DAY_OF_WEEK\"] == \"Sun\", \"07\")\n",
    "                     .otherwise(df1[\"DAY_OF_WEEK\"]))\n",
    "\n",
    "# Create full DATE column\n",
    "df1 = df1.withColumn(\"MONTH\", df1[\"MONTH\"].cast(\"string\"))\n",
    "df1 = df1.withColumn(\"Day\", df1[\"Day\"].cast(\"string\"))\n",
    "df1 = df1.withColumn(\"Year\", df1[\"Year\"].cast(\"string\"))\n",
    "df1 = df1.withColumn(\"Time\", df1[\"Time\"].cast(\"string\"))\n",
    "df1 = df1.withColumn(\"DATE\", concat(\"Year\", lit(\"-\"), \"MONTH\", lit(\"-\"), \"Day\", lit(\" \"), \"Time\"))\n",
    "df1 = df1.withColumn(\"DATE\", to_timestamp(df1[\"DATE\"], \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Drop columns\n",
    "df1 = df1.select(\"Indx\", \"ID\", \"DATE\", \"DAY_OF_WEEK\", \"TWEET\")\n",
    "\n",
    "# Clean tweets\n",
    "df1 = df1.withColumn(\"TWEET\", lower(col(\"TWEET\")))\n",
    "df1 = df1.withColumn(\"TWEET\", F.regexp_replace(col(\"TWEET\"), r'@\\w+', ''))\n",
    "df1 = df1.withColumn(\"TWEET\", F.regexp_replace(col(\"TWEET\"), r'http\\S+', ''))\n",
    "df1 = df1.withColumn(\"TWEET\", \n",
    "                     regexp_replace(col(\"TWEET\"), r'[^\\w\\s]|quot|amp|\\bwww\\.\\S+|\\d+', ''))\n",
    "\n",
    "# Count tweets with more than 140 characters\n",
    "df1 = df1.withColumn(\"tweet_length\", length(\"TWEET\"))\n",
    "print('Tweets with more than 140 characters:', df1.filter(length(\"TWEET\") > 140).count())\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5db5a028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Indx: integer (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- DATE: timestamp (nullable = true)\n",
      " |-- DAY_OF_WEEK: string (nullable = true)\n",
      " |-- TWEET: string (nullable = true)\n",
      " |-- tweet_length: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d2db1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de tweets con más de 140 caracteres: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-------------------+-----------+--------------------+------------+\n",
      "|Indx|        ID|               DATE|DAY_OF_WEEK|               TWEET|tweet_length|\n",
      "+----+----------+-------------------+-----------+--------------------+------------+\n",
      "|   0|1467810369|2009-04-06 22:19:45|         01|   awww thats a b...|          74|\n",
      "|   1|1467810672|2009-04-06 22:19:49|         01|is upset that he ...|         105|\n",
      "|   2|1467810917|2009-04-06 22:19:53|         01| i dived many tim...|          76|\n",
      "|   3|1467811184|2009-04-06 22:19:57|         01|my whole body fee...|          47|\n",
      "|   4|1467811193|2009-04-06 22:19:57|         01| no its not behav...|          87|\n",
      "+----+----------+-------------------+-----------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Clean tweets\n",
    "df1 = df1.withColumn(\"TWEET\", lower(col(\"TWEET\")))\n",
    "df1 = df1.withColumn(\"TWEET\", F.regexp_replace(col(\"TWEET\"), r'@\\w+', ''))\n",
    "df1 = df1.withColumn(\"TWEET\", F.regexp_replace(col(\"TWEET\"), r'http\\S+', ''))\n",
    "df1 = df1.withColumn(\"TWEET\", \n",
    "                     regexp_replace(col(\"TWEET\"), r'[^\\w\\s]|quot|amp|\\bwww\\.\\S+|\\d+', ''))\n",
    "\n",
    "# Count tweets with more than 140 characters\n",
    "df1 = df1.withColumn(\"tweet_length\", length(\"TWEET\"))\n",
    "print('Total de tweets con más de 140 caracteres:', df1.filter(length(\"TWEET\") > 140).count())\n",
    "#counts = counts_df.filter(counts_df[\"tweet_length\"] > 140).count()\n",
    "#print('Total de tweets con más de 140 caracteres:', counts)\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136435fe",
   "metadata": {},
   "source": [
    "# Write DF in a cvs file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afdf3f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Store in Hadoop\n",
    "df1.write.mode(\"overwrite\").option(\"header\", True).csv('/CA2/clean_tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebaf9421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Store local\n",
    "df1.coalesce(1).write.option(\"header\", True).csv('file:///home/hduser/Downloads/clean_tweets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25ff671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
